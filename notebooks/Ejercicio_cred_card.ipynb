{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "## Anonymized credit card transactions labeled as fraudulent or genuine\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "\n",
    "Objetivo: Entrenar un clasificador para detectar operaciones fraudulentas.\n",
    "\n",
    "Se estima que al año se realizan 100 **mil millones** de transacciones con CC.\n",
    "\n",
    "\n",
    "Obtenemos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-11 10:34:28--  https://github.com/israelzuniga/dlatam-bigdata-workshop/raw/master/notebooks/data/creditcardfraud.zip\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/creditcardfraud.zip [following]\n",
      "--2017-11-11 10:34:29--  https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/creditcardfraud.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 71387154 (68M) [application/zip]\n",
      "Saving to: ‘creditcardfraud.zip’\n",
      "\n",
      "creditcardfraud.zip 100%[=====================>]  68.08M  3.39MB/s   in 20s    \n",
      "\n",
      "2017-11-11 10:34:51 (3.41 MB/s) - ‘creditcardfraud.zip’ saved [71387154/71387154]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/israelzuniga/dlatam-bigdata-workshop/raw/master/notebooks/data/creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  creditcardfraud.zip\n",
      "  inflating: creditcard.csv          \n"
     ]
    }
   ],
   "source": [
    "!unzip creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos constantes para re-usarse en la app de Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CSV_PATH = \"./creditcard.csv\"\n",
    "APP_NAME = \"Random Forest Example\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 13579\n",
    "TRAINING_DATA_RATIO = 0.7\n",
    "RF_NUM_TREES = 3\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32\n",
    "RF_MAX_BINS = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 284807\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .master(SPARK_URL) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferschema = \"true\") \\\n",
    "    .csv(CSV_PATH)\n",
    "\n",
    "print(\"Rows: %d\" % df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que cargamos el CSV, necesitamos transformarlo a un vector y separar datos para entrenamiento y pruebas:\n",
    "\n",
    "https://spark.apache.org/docs/1.1.0/mllib-data-types.html\n",
    "\n",
    "https://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#module-pyspark.mllib.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set rows: 199690\n",
      "Number of test set rows: 85117\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "transformed_df = df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "print(\"Training set rows: %d\" % training_data.count())\n",
    "print(\"Test set rows: %d\" % test_data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 23.653 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from time import *\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "model = RandomForest.trainClassifier(training_data, numClasses=2, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 99.942%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\n",
    "acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under Precision/Recall (PR) curve: 80\n",
      "Area under Receiver Operating Characteristic (ROC) curve: 91.802\n",
      "Time to evaluate model: 18.353 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labels_and_predictions)\n",
    "print(\"Area under Precision/Recall (PR) curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "print(\"Area under Receiver Operating Characteristic (ROC) curve: %.3f\" % (metrics.areaUnderROC * 100))\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
