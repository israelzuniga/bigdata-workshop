{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning en Spark\n",
    "\n",
    "## Algoritmos disponibles para:\n",
    "### - Clasficación (Sup)\n",
    "### - Regresión (Sup)\n",
    "### - Collaborative Filtering (N.S.)\n",
    "### - Clustering (N.S.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "## Spark MLlib\n",
    "La implementación SparkMLlib (disponible desde Spark 0.8) provee funciones de ML para usarse en RDDs. MLlib, al igual que Spark Streaming y Spark SQL son componentes integrales de Spark.\n",
    "\n",
    "\n",
    "## Spark ML\n",
    "A partir de Spark 1.2, una librería adicional, Spark ML, se introduce para extender MLlib a DataFrames de Spark SQL. Las APIs, algoritmos y otras funciones están basadas en MLlib. Es la primera elección si trabajamos con SparkSQL.\n",
    "\n",
    "Los conceptos e implementaciones son intercambiables entre ambas APIs.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n",
    "\n",
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([ 1,2,3,4,5,6,7,8,9,10]) \n",
    "training, test = data.randomSplit([ 0.6, 0.4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint \n",
    "\n",
    "\n",
    "# Golf Dataset: https://gerardnico.com/wiki/data_mining/weather\n",
    "outlook = {\" sunny\": 0.0, \" overcast\": 1.0, \" rainy\": 2.0} \n",
    "\n",
    "labeledpoints = [ \n",
    "    LabeledPoint( 0.0, [outlook[\" sunny\"], 85,85, False]), \n",
    "    LabeledPoint( 0.0, [outlook[\" sunny\"], 80,90, True]), \n",
    "    LabeledPoint( 1.0, [outlook[\" overcast\"], 83,86, False]),\n",
    "    LabeledPoint( 1.0, [outlook[\" rainy\"], 70,96, False]), \n",
    "    LabeledPoint( 1.0, [outlook[\" rainy\"], 68,80, False]), \n",
    "    LabeledPoint( 0.0, [outlook[\" rainy\"], 65,70, True]), \n",
    "    LabeledPoint( 1.0, [outlook[\" overcast\"], 64,65, True]), \n",
    "    LabeledPoint( 0.0, [outlook[\" sunny\"], 72,95, False]), \n",
    "    LabeledPoint( 1.0, [outlook[\" sunny\"], 69,70, False]), \n",
    "    LabeledPoint( 1.0, [outlook[\" sunny\"], 75,80, False]), \n",
    "    LabeledPoint( 1.0, [outlook[\" sunny\"], 75,70, True]), \n",
    "    LabeledPoint( 1.0, [outlook[\" overcast\"], 72,90, True]), \n",
    "    LabeledPoint( 1.0, [outlook[\" overcast\"], 81,75, False]), \n",
    "    LabeledPoint( 0.0, [outlook[\" rainy\"], 71,91, True]) ] \n",
    "\n",
    "\n",
    "data = sc.parallelize( labeledpoints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree \n",
    "\n",
    "model = DecisionTree.trainClassifier( data = data, numClasses = 2, categoricalFeaturesInfo ={ 0: 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.numNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict([ 1.0,85,85, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel \n",
    "\n",
    "\n",
    "model = NaiveBayes.train( data = data, lambda_ = 1.0) \n",
    "\n",
    "\n",
    "\n",
    "model.predict([ 1.0,85,85, True])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering (AKA Matrix Factorization)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/sty-spark/movielens/movielens.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"movielens.dat\") \n",
    "\n",
    "\n",
    "ratings = data.map(lambda x: x.split('\\ t'))\n",
    "\n",
    "ratings = ratings.map(lambda x: Rating( int( x[ 0]), int( x[ 1]), float( x[ 2])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rank = 10 \n",
    "# numIterations = 10 \n",
    "# model = ALS.train(ratings, rank, numIterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata = ratings.map( lambda p: (p[ 0], p[ 1])) \n",
    "\n",
    "predictions = model.predictAll( testdata) \\\n",
    "    .map( lambda r: (( r[ 0], r[ 1]), r[ 2])) \n",
    "    \n",
    "ratesAndPreds = ratings.map( lambda r: (( r[ 0], r[ 1]), r[ 2])) \\\n",
    "    .join( predictions) \n",
    "    \n",
    "MSE = ratesAndPreds.map( lambda r: (r[ 1][ 0] - r[ 1][ 1])** 2) \\\n",
    "    .mean() \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\" Mean Squared Error = \" + str( MSE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel \n",
    "\n",
    "from numpy import array \n",
    "\n",
    "from math import sqrt\n",
    "data = sc.textFile(\"/usr/local/spark/data/mllib/kmeans_data.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsedData = data.map( lambda line: array( \\\n",
    "                                          [float( x) for x in line.split(' ')])) \n",
    "# Build the model (cluster the data) \n",
    "clusters = KMeans.train( parsedData, 2, maxIterations = 10, runs = 10, initializationMode =\" random\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors \n",
    "def error( point): \n",
    "    center = clusters.centers[ clusters.predict( point)]\n",
    "    return sqrt( sum([ x** 2 for x in (point - center)])) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WSSSE = parsedData.map( lambda point: error( point)) \\\n",
    "    .reduce( lambda x, y: x + y) \n",
    "    \n",
    "    \n",
    "print(\" Within Set Sum of Squared Error = \" + str( WSSSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters.save(path='./my_clusters_model.spark_model', sc=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "cluster_izm = KMeansModel.load(sc, path='./my_clusters_model.spark_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(cluster_izm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
