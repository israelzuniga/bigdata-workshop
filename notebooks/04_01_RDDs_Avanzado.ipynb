{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD storage levels\n",
    "- Caching y persistencia distribuida de RDDs\n",
    "- RDDs Checkpointing\n",
    "- Escritura de RDD a archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD lineage\n",
    "\n",
    "![RDD lineage](https://github.com/israelzuniga/dlatam-bigdata-workshop/blob/master/notebooks/img/rdd_lineage.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/lorem.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"RDDS\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\\\n",
    "        .setMaster(SPARK_URL)\\\n",
    "        .setAppName(APP_NAME))\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext(conf= conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: lorem.txt\n",
    "lorem = sc.textFile('lorem.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD: lorem\n",
    "\n",
    "words = lorem.flatMap(lambda x: x.split())\n",
    "\n",
    "\n",
    "words.take(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD: words\n",
    "\n",
    "longwords = words.filter(lambda x: len(x) > 5)\n",
    "\n",
    "\n",
    "\n",
    "longwords.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD: longwords\n",
    "\n",
    "numwords = longwords.count()\n",
    "\n",
    "\n",
    "print(numwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longwords.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acción `longwords.count()` obliga la evaluación de los RDDs padres hasta `longwords`. Si esta acción (o cualquier otra como `longwords.take(6)` o `longwords.collect()`) es llamada en una ocasión posterior, el linaje entero se reevualua . En casos simples, con datos pequeños en una o dos fases, las reevaluaciones no son problema. Pero en muchas circunstancias pueden ser ineficientes y pueden impactar el tiempo de recuperación en caso de catástrofe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Storage Levels\n",
    "\n",
    "\n",
    "Storage Level|Desc\n",
    "------------|------\n",
    "MEMORY_ONLY|(Default) RDD partitions are stored in memory only.\n",
    "MEMORY_AND_DISK| RDD partitions that do no fit in memory are stored in disk.\n",
    "MEMORY_ONLY_SER| RDD partitions are stored as serialized objects in memory. This option can be used to save memory (as serialized objects may consume less space than their deserialized equvalent).\n",
    "MEMORY_AND_DISK_SER| RDD partitions are stored as serialized objects in memory. Objects that do not fit into memory are spilled to disk.\n",
    "DISK_ONLY| RDD partitions are stored on disk only.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Storage Level Flags\n",
    "\n",
    "### StorageClass Constructor\n",
    "```\n",
    "StorageLevel(useDisk,\n",
    "              useMemory,\n",
    "              useOffHeap,\n",
    "              deserialized,\n",
    "              replication=1)\n",
    "```\n",
    "\n",
    "\n",
    "`useDisk`, `useMemory`, `useOffHeap`, y `deserialized` son argumentos Booleanos, mientras que `replication` es de valor entero (default a 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark API: `getStorageLevel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_sl = lorem.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_sl.useDisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_sl.useMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligiendo un nivel de almacenamiento:\n",
    "\n",
    "El nivel de almacenamiento de los RDD permiten ajustar el funcionamiento de los trabajos en Spark y acomodar operaciones que de otra forma no tendrían espacio en la memoria del cluster. Adicionalmente, las opciones disponibles de replicación pueden reducir el tiempo de restauraación en caso de fallas.\n",
    "\n",
    "Generalmente hablando, si un RDD cabe en la memoria disponible del cluster, el nivel de almacenamiento por default es suficiente y proveerá el mejor rendimiento.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Caching / Persistence / Checkpointing\n",
    "\n",
    "\n",
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/all-shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sc.textFile(\"all-shakespeare.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.flatMap( lambda x: x.split()) \\\n",
    "    .map( lambda x: (x, 1))\\\n",
    "    .reduceByKey( lambda x, y: x + y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cache() \n",
    "words.count() # triggers computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.take( 30) # no computation required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.count() # no computation required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence `persist()`\n",
    "\n",
    "```python\n",
    "# Default\n",
    "RDD.persist(storageLevel =  StorageLevel.MEMORY_ONLY_SER)\n",
    "\n",
    "# myrdd\n",
    "myrdd.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n",
    "\n",
    "# =\n",
    "myrdd.persist(StorageLevel(True, True, False, False, 2))\n",
    "```\n",
    "\n",
    "\n",
    "##  `unpersist()`\n",
    "Si el RDD deja de requerir perssitencia en disco, usamos este método. También si queremos cambiar las propiedades de persistencia de un RDD, debemos abandonar su configuración y especificarla posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sc.textFile(\"all-shakespeare.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.flatMap( lambda x: x.split()) \\\n",
    "    .map( lambda x: (x, 1))\\\n",
    "    .reduceByKey( lambda x, y: x + y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.persist(storageLevel=StorageLevel(False, True, False, False, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.persist(storageLevel=StorageLevel(False, True, False, False, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing `checkpoint()`\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "SparkContext.setCheckpointDir()\n",
    "\n",
    "\n",
    "RDD.checkpoint()\n",
    "\n",
    "\n",
    "RDD.isCheckpointed()\n",
    "\n",
    "\n",
    "RDD.getCheckpointFile()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('losrudos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sc.textFile('all-shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.flatMap( lambda x: x.split()) \\\n",
    "    .map( lambda x: (x, 1))\\\n",
    "    .reduceByKey( lambda x, y: x + y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.isCheckpointed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.getCheckpointFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: Checkpointing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
