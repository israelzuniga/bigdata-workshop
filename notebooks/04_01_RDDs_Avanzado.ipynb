{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD storage levels\n",
    "- Caching y persistencia distribuida de RDDs\n",
    "- RDDs Checkpointing\n",
    "- Escritura de RDD a archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD lineage\n",
    "\n",
    "![RDD lineage](https://github.com/israelzuniga/dlatam-bigdata-workshop/blob/master/notebooks/img/rdd_lineage.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-03 15:33:13--  https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/lorem.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29769 (29K) [text/plain]\n",
      "Saving to: ‘lorem.txt.1’\n",
      "\n",
      "lorem.txt.1         100%[===================>]  29.07K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2018-11-03 15:33:13 (1.05 MB/s) - ‘lorem.txt.1’ saved [29769/29769]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/lorem.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"RDDS\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\\\n",
    "        .setMaster(SPARK_URL)\\\n",
    "        .setAppName(APP_NAME))\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext(conf= conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: lorem.txt\n",
    "lorem = sc.textFile('lorem.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'ipsum',\n",
       " 'dolor',\n",
       " 'sit',\n",
       " 'amet,',\n",
       " 'consectetur',\n",
       " 'adipiscing',\n",
       " 'elit.',\n",
       " 'Phasellus',\n",
       " 'ante',\n",
       " 'enim,',\n",
       " 'sagittis']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDD: lorem\n",
    "\n",
    "words = lorem.flatMap(lambda x: x.split())\n",
    "\n",
    "\n",
    "words.take(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consectetur',\n",
       " 'adipiscing',\n",
       " 'Phasellus',\n",
       " 'sagittis',\n",
       " 'pellentesque',\n",
       " 'vulputate']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD: words\n",
    "\n",
    "longwords = words.filter(lambda x: len(x) > 5)\n",
    "\n",
    "\n",
    "\n",
    "longwords.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2227\n"
     ]
    }
   ],
   "source": [
    "# RDD: longwords\n",
    "\n",
    "numwords = longwords.count()\n",
    "\n",
    "\n",
    "print(numwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[5] at RDD at PythonRDD.scala:49 []\\n |  lorem.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  lorem.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "print(longwords.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acción `longwords.count()` obliga la evaluación de los RDDs padres hasta `longwords`. Si esta acción (o cualquier otra como `longwords.take(6)` o `longwords.collect()`) es llamada en una ocasión posterior, el linaje entero se reevualua . En casos simples, con datos pequeños en una o dos fases, las reevaluaciones no son problema. Pero en muchas circunstancias pueden ser ineficientes y pueden impactar el tiempo de recuperación en caso de catástrofe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Storage Levels\n",
    "\n",
    "\n",
    "Storage Level|Desc\n",
    "------------|------\n",
    "MEMORY_ONLY|(Default) RDD partitions are stored in memory only.\n",
    "MEMORY_AND_DISK| RDD partitions that do no fit in memory are stored in disk.\n",
    "MEMORY_ONLY_SER| RDD partitions are stored as serialized objects in memory. This option can be used to save memory (as serialized objects may consume less space than their deserialized equvalent).\n",
    "MEMORY_AND_DISK_SER| RDD partitions are stored as serialized objects in memory. Objects that do not fit into memory are spilled to disk.\n",
    "DISK_ONLY| RDD partitions are stored on disk only.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Storage Level Flags\n",
    "\n",
    "### StorageClass Constructor\n",
    "```\n",
    "StorageLevel(useDisk,\n",
    "              useMemory,\n",
    "              useOffHeap,\n",
    "              deserialized,\n",
    "              replication=1)\n",
    "```\n",
    "\n",
    "\n",
    "`useDisk`, `useMemory`, `useOffHeap`, y `deserialized` son argumentos Booleanos, mientras que `replication` es de valor entero (default a 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark API: `getStorageLevel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lorem.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_sl = lorem.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lorem.getStorageLevel().useDisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lorem_sl.useDisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lorem_sl.useMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligiendo un nivel de almacenamiento:\n",
    "\n",
    "El nivel de almacenamiento de los RDD permiten ajustar el funcionamiento de los trabajos en Spark y acomodar operaciones que de otra forma no tendrían espacio en la memoria del cluster. Adicionalmente, las opciones disponibles de replicación pueden reducir el tiempo de restauración en caso de fallas.\n",
    "\n",
    "Generalmente hablando, si un RDD cabe en la memoria disponible del cluster, el nivel de almacenamiento por default es suficiente y proveerá el mejor rendimiento.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Caching / Persistence / Checkpointing\n",
    "\n",
    "\n",
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-03 15:44:44--  https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/all-shakespeare.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5342761 (5.1M) [text/plain]\n",
      "Saving to: ‘all-shakespeare.txt.1’\n",
      "\n",
      "all-shakespeare.txt 100%[===================>]   5.09M  2.39MB/s    in 2.1s    \n",
      "\n",
      "2018-11-03 15:44:46 (2.39 MB/s) - ‘all-shakespeare.txt.1’ saved [5342761/5342761]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/israelzuniga/dlatam-bigdata-workshop/master/notebooks/data/all-shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sc.textFile(\"all-shakespeare.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.flatMap( lambda x: x.split()) \\\n",
    "    .map( lambda x: (x, 1))\\\n",
    "    .reduceByKey( lambda x, y: x + y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73032"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.cache() \n",
    "words.count() # triggers computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 48),\n",
       " ('IV', 501),\n",
       " ('PERSONAE', 37),\n",
       " ('Fourth.', 3),\n",
       " ('(KING', 15),\n",
       " ('HENRY,', 16),\n",
       " ('Prince', 78),\n",
       " ('of', 16526),\n",
       " ('HENRY:)', 2),\n",
       " ('|', 626),\n",
       " ('Lancaster', 21),\n",
       " ('(LANCASTER:)', 1),\n",
       " ('SIR', 486),\n",
       " ('WALTER', 21),\n",
       " ('BLUNT:', 2),\n",
       " ('PERCY', 35),\n",
       " ('OF', 1071),\n",
       " ('(NORTHUMBERLAND:)', 4),\n",
       " ('HOTSPUR,', 6),\n",
       " ('his', 6712),\n",
       " ('(HOTSPUR:)', 1),\n",
       " ('MORTIMER', 29),\n",
       " ('(MORTIMER:)', 2),\n",
       " ('RICHARD', 409),\n",
       " ('ARCHIBALD', 1),\n",
       " ('VERNON', 32),\n",
       " ('FALSTAFF', 492),\n",
       " ('MICHAEL', 10),\n",
       " ('POINS:', 2),\n",
       " ('GADSHILL:', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take( 30) # no computation required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count() # no computation required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence `persist()`\n",
    "\n",
    "```python\n",
    "# Default\n",
    "RDD.persist(storageLevel =  StorageLevel.MEMORY_ONLY_SER)\n",
    "\n",
    "# myrdd\n",
    "myrdd.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n",
    "\n",
    "# =\n",
    "myrdd.persist(StorageLevel(True, True, False, False, 2))\n",
    "```\n",
    "\n",
    "\n",
    "##  `unpersist()`\n",
    "Si el RDD deja de requerir perssitencia en disco, usamos este método. También si queremos cambiar las propiedades de persistencia de un RDD, debemos abandonar su configuración y especificarla posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sc.textFile(\"all-shakespeare.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.flatMap( lambda x: x.split()) \\\n",
    "    .map( lambda x: (x, 1))\\\n",
    "    .reduceByKey( lambda x, y: x + y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[22] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73032"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 48), ('IV', 501), ('PERSONAE', 37)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[22] at RDD at PythonRDD.scala:49 [Memory Serialized 1x Replicated]\\n |       CachedPartitions: 2; MemorySize: 1380.9 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\\n |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:129 [Memory Serialized 1x Replicated]\\n |  ShuffledRDD[20] at partitionBy at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\\n +-(2) PairwiseRDD[19] at reduceByKey at <ipython-input-21-bb809d795f06>:1 [Memory Serialized 1x Replicated]\\n    |  PythonRDD[18] at reduceByKey at <ipython-input-21-bb809d795f06>:1 [Memory Serialized 1x Replicated]\\n    |  all-shakespeare.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\\n    |  all-shakespeare.txt HadoopRDD[16] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]'\n"
     ]
    }
   ],
   "source": [
    "print(words.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o140.persist.\n: java.lang.UnsupportedOperationException: Cannot change storage level of an RDD after it was already assigned a level\n\tat org.apache.spark.rdd.RDD.persist(RDD.scala:170)\n\tat org.apache.spark.rdd.RDD.persist(RDD.scala:195)\n\tat org.apache.spark.api.java.JavaRDD.persist(JavaRDD.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-86ae282b536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mpersist\u001b[0;34m(self, storageLevel)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mjavaStorageLevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getJavaStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjavaStorageLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o140.persist.\n: java.lang.UnsupportedOperationException: Cannot change storage level of an RDD after it was already assigned a level\n\tat org.apache.spark.rdd.RDD.persist(RDD.scala:170)\n\tat org.apache.spark.rdd.RDD.persist(RDD.scala:195)\n\tat org.apache.spark.api.java.JavaRDD.persist(JavaRDD.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "words.persist(storageLevel=StorageLevel(False, True, False, False, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[22] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[22] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.persist(storageLevel=StorageLevel(True, True, True, False, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[22] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing `checkpoint()`\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "SparkContext.setCheckpointDir()\n",
    "\n",
    "\n",
    "RDD.checkpoint()\n",
    "\n",
    "\n",
    "RDD.isCheckpointed()\n",
    "\n",
    "\n",
    "RDD.getCheckpointFile()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('claseNoviembre/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sc.textFile('all-shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.flatMap( lambda x: x.split()) \\\n",
    "    .map( lambda x: (x, 1))\\\n",
    "    .reduceByKey( lambda x, y: x + y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73032"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.isCheckpointed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/notebooks/claseNoviembre/1ab0f255-b717-4627-ad74-71f5d85cbb77/rdd-31'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getCheckpointFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: Checkpointing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
