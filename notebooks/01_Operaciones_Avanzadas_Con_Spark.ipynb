{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# k/v RDDs\n",
    "\n",
    "## Trabajando con pares llave/valor    (key/value, k/v)\n",
    "\n",
    "### - De forma natural, bastantes entidades se pueden representar como llaves:\n",
    "    - p. ej., tiempo de eventos (timestamp), id de cliente, etc.\n",
    "### - En Python, usamos tuplas para formar pares k/v\n",
    "```python\n",
    ">>> (\"taquitos\", 4), (\"burritos\", 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Pair RDDs\n",
    "\n",
    "### A aquellos RDDs que contienen pares k/v los llamamos  **Pair RDDs** y están compuestos de tuplas:\n",
    "```python\n",
    "pairs = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"c\", 6)])\n",
    "```\n",
    "\n",
    "### Spark ofrece operaciones especiales en **Pair RDDs**:\n",
    "    - reduceByKey, sortByKey, joins\n",
    "    - Son funciones que solo operan en tuplas\n",
    "### Los Pair RDDs también soportan las mismas funciones que RDDs regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = \"local[*]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## reduceByKey()\n",
    "\n",
    "### - Ejecuta multiples operaciones de reducción (reduce ops); una por cada llave en el dataset\n",
    "### - Cada reducción combina valores que tienen la misma llave\n",
    "### - reduceByKey() no es una acción como reduce()\n",
    "### - Regresa un RDD nuevo (y no un valor)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    ">> rdd = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"b\", 6)])\n",
    "\n",
    ">> rdd.reduceByKey(lambda x, y: x + y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"b\", 6)])\n",
    "\n",
    "rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## sortByKey()\n",
    "\n",
    "### - Regresa un RDD ordenado según la llave\n",
    "\n",
    "### - `sortByKey(ascending = True/False, keyFunc = lambda... )`\n",
    "    - Con ordenes ascendentes/descendientes\n",
    "    - Se puede proveer funciones de comparación\n",
    "    \n",
    "    \n",
    "    \n",
    "```python\n",
    "\n",
    ">> rdd = sc.parallelize([('c', 2), ('a', 3), ('b', 1)])\n",
    "\n",
    ">> rdd.sortByKey()\n",
    "\n",
    ">> rdd.sortByKey(False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('c', 2), ('a', 3), ('b', 1)])\n",
    "\n",
    "rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd.sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## groupByKey()\n",
    "\n",
    "```python\n",
    "\n",
    ">> rdd = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"b\", 6)])\n",
    "\n",
    ">> rdd.groupByKey()\n",
    "\n",
    ">>> [('b', [4, 6]), ('a', [2])]\n",
    "```\n",
    "\n",
    "### - 💅🏼🔥 groupByKey() puede causar una sobre-carga de datasets a ordenarse entre workers! 🙈🚒\n",
    "### - [Avoid GroupByKey: Databricks Spark Knowledge Base](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"b\", 6)])\n",
    "\n",
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## mapValues()\n",
    "\n",
    "### - Retorna un nuevo RDD al aplicar una función a cada valor del RDD original\n",
    "\n",
    "\n",
    "```python\n",
    ">> rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"b\", 3)])\n",
    "\n",
    ">> rdd.mapValues(lambda x: x * 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"b\", 3)])\n",
    "\n",
    "rdd.mapValues(lambda x: x * 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pair RDD actions\n",
    "\n",
    "### - Todas las acciones de RDDs básicos están disponibles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## countByKey()\n",
    "\n",
    "### - Conteo de número de incidencias para cada llave\n",
    "\n",
    "```python\n",
    ">> rdd = sc.parallelize([('a', \"doc1\"), ('b', 'doc1'), ('b', 'doc2')])\n",
    "\n",
    ">> rdd.countByKey()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a', \"doc1\"), ('b', 'doc1'), ('b', 'doc2')])\n",
    "\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collectAsMap()\n",
    "\n",
    "### - Búsqueda mediante un diccionario\n",
    "\n",
    "```python\n",
    ">> rdd = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"c\", 6)])\n",
    "\n",
    ">> dd = rdd.collectAsMap()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 2), (\"b\", 4), (\"c\", 6)])\n",
    "\n",
    "dd = rdd.collectAsMap()\n",
    "\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dd['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Partitions\n",
    "\n",
    "### - Cada RDD siempre se separa en un número de particiones\n",
    "\n",
    "### - El grado de paralelismo es determinado por el número de particiones\n",
    "- [Parallelism vs Concurrency](https://wiki.haskell.org/Parallelism_vs._Concurrency)\n",
    "\n",
    "### - rdd.getNumberOfPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RDD Partitions\n",
    "![RDD Partitions](https://github.com/israelzuniga/dlatam-bigdata-workshop/blob/master/notebooks/img/rdd_partitions.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cómo definir el número de particiones?\n",
    "\n",
    "### - ```sc.textFile(path, minPartitions=None)```\n",
    "\n",
    "\n",
    "### - repartition() puede ser usada para un nuevo dataset con otro número de particiones\n",
    "\n",
    "### - Útil cuando se conoce previamente la cantidad de llaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = sc.textFile('README.md', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x.repartition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data partitioning\n",
    "\n",
    "\n",
    "### - El particionamiento determina como se agrupan las llaves en el mismo nodo\n",
    "\n",
    "### - Útil para operaciones como reduceByKey, joins, cogroup, etc...\n",
    "\n",
    "### - Reducen la comunicación de red entre nodos 💰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No olvidar:\n",
    "\n",
    "### - Algunas operaciones no preservan el particionamiento (p. ej. map())\n",
    "\n",
    "\n",
    "### - Se debe usar mapValues() o flatMapValues() en vez de map(), siempre que no se necesite cambiar la llave\n",
    "\n",
    "### - Es recomendable usar partitionBy() antes de operaciones tipo join y reduce en datasets extensos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Joins\n",
    "\n",
    "\n",
    "## - pySpark soporta cuatro tipos de uniones en RDDs:\n",
    "\n",
    "###  ```join(), leftOuterJoin(), rightOuterJoin(), fullOuterJoin()```\n",
    "\n",
    "## - Su uso es más común en Pair RDDs\n",
    "\n",
    "### - P. Ej.: Combinar un dataset de clientes con otro dataset de compras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join()\n",
    "\n",
    "### join() funciona de la misma manera que un \"inner join\" en SQL\n",
    "\n",
    "### Solo las llaves presentes en ambos RDDs se preservan\n",
    "\n",
    "### Retorna todos los pares de elementos como tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leftOuterJoin\n",
    "\n",
    "### ``` x.leftOuterJoin(y) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('a', 1), ('b', 4)])\n",
    "\n",
    "y = sc.parallelize([('a', 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.leftOuterJoin(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rightOuterJoin\n",
    "\n",
    "### ``` y.leftOuterJoin(x) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('a', 1), ('b', 4)])\n",
    "\n",
    "y = sc.parallelize([('a', 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.rightOuterJoin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fullOuterJoin()\n",
    "\n",
    "### ``` x.fullOuterJoin(y) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('a', 1), ('b', 4)])\n",
    "\n",
    "y = sc.parallelize([('a', 2), ('c', 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.fullOuterJoin(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Review:\n",
    "\n",
    "- Pair RDDs\n",
    "\n",
    "- Partitions\n",
    "\n",
    "- Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
