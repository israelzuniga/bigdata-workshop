{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variables compartidas en Spark:\n",
    "    - Broadcast variables\n",
    "    - Accumulators\n",
    "- Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables, son variables solo de lectura grabadas por nuestro Spark Driver y puestas a disposici贸n de todos los nodos de trabajo en el cluster.\n",
    "Esta distribuci贸n de informaci贸n las hace disponibles a cualquier tarea corriendo en los ejecutores de los nodos. Se comparten a trav茅s de un protocolo P2P (basado en BitTorrent / BitTorrent Broadcast), lo que proporciona gran escalabilidad simplemente al ser enviadas mediante el Spark Driver.\n",
    "\n",
    "![broadcast vars](https://github.com/israelzuniga/dlatam-bigdata-workshop/blob/be9dbf6b5fcf4f0b32c2f490a4045a5f2c6daa3c/notebooks/img/broadcast_vars.png?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Performance and Scalability of Broadcast in Spark, Mosharaf Chowdhury. University of California, Berkeley:**\n",
    "\n",
    "[Performance and Scalability of Broadcast in Spark](https://pdfs.semanticscholar.org/7b0e/6a3dc18babb19daddb63890e763795943485.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"API_Avanzado\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\\\n",
    "        .setMaster(SPARK_URL)\\\n",
    "        .setAppName(APP_NAME))\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext(conf= conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C贸mo crear y usar variables de broadcast? `broadcast()`\n",
    "\n",
    "\n",
    "`SparkContext.broadcast(value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = sc.broadcast({'83': 'Mezes Park', '84':'Dolores Park'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7efc1b1e74e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aunque podemos instanciar variables de broadcast a partir de archivos; su posible uso se traslapa con la funci贸n b谩sica del RDD** \n",
    "\n",
    "\n",
    "\n",
    "Una vez que tenemos objetos de tipo broadcast, los podemos usar con la siguiente sintaxis:\n",
    "\n",
    "`Clase: value()`\n",
    "\n",
    "`Broadcast.value()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dolores Park'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.value['84']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "`unpersist()`\n",
    "\n",
    "Igual que la funci贸n anterior de persistencia para RDDs.\n",
    "\n",
    "`Broadcast.unpersist(blocking=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cu谩les son las ventajas de las variables de Broadcast ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "# Accumulators\n",
    "\n",
    "Otro tipo de variables compartidas en Spark, son las variables de tipo acumulador. A diferencia del tipo Broadcast, estas pueden actualizar su valor. Especificamente son valores num茅ricos (enteros o decimales) que se pueden incrementar.\n",
    "\n",
    "Se escriben inicialmente por nuestro Spark Driver y su actualizaci贸n corresponde a las tareas en los nodos. El valor final del acumulador puede ser obtenido de regreso, generalmente al acabar la app de Spark.\n",
    "\n",
    "Se actualizan solamente una vez por tarea exitosa. Los nodos envian las actualizaciones hacia el Spark Driver, que es el 煤nico proceso que puede leer su valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plus_one(x):\n",
    "    global acc\n",
    "    acc += 1\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myrdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.map(lambda x: plus_one(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros procesados: 5\n"
     ]
    }
   ],
   "source": [
    "print('Registros procesados: ' + str(acc.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.map(lambda x: plus_one(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros procesados: 10\n"
     ]
    }
   ],
   "source": [
    "print('Registros procesados: ' + str(acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accumulators `Obj: AccumulatorParam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        dict1 = {}\n",
    "        for i in range(0, len(value)):\n",
    "            dict1[i] = 0\n",
    "        return dict1\n",
    "    # next funct\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in val1.keys():\n",
    "            val1[i] += val2[i]\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([{ 0: 0.3, 1: 0.8, 2: 0.4}, {0: 0.2, 1: 0.4, 2: 0.2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_acc = sc.accumulator({0: 0, 1: 0, 2: 0}, VectorAccumulatorParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapping_fn(x):\n",
    "    global vector_acc\n",
    "    vector_acc += x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1.foreach(mapping_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5, 1: 1.2000000000000002, 2: 0.6000000000000001}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usos para variables tipo Accumulator\n",
    "\n",
    "Generalmente los usaremos para prop贸sitos operacionales, como:\n",
    "\n",
    "- conteo de registros procesados\n",
    "- seguimiento de registros malformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
