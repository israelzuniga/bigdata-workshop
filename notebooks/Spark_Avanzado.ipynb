{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variables compartidas en Spark:\n",
    "    - Broadcast variables\n",
    "    - Accumulators\n",
    "- Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables, son variables solo de lectura grabadas por nuestro Spark Driver y puestas a disposición de todos los nodos de trabajo en el cluster.\n",
    "Esta distribución de información las hace disponibles a cualquier tarea corriendo en los ejecutores de los nodos. Se comparten a través de un protocolo P2P (basado en BitTorrent / BitTorrent Broadcast), lo que proporciona gran escalabilidad simplemente al ser enviadas mediante el Spark Driver.\n",
    "\n",
    "![broadcast vars](https://github.com/israelzuniga/dlatam-bigdata-workshop/blob/be9dbf6b5fcf4f0b32c2f490a4045a5f2c6daa3c/notebooks/img/broadcast_vars.png?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Performance and Scalability of Broadcast in Spark, Mosharaf Chowdhury. University of California, Berkeley:**\n",
    "\n",
    "[Performance and Scalability of Broadcast in Spark](https://pdfs.semanticscholar.org/7b0e/6a3dc18babb19daddb63890e763795943485.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"API_Avanzado\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\\\n",
    "        .setMaster(SPARK_URL)\\\n",
    "        .setAppName(APP_NAME))\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext(conf= conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo crear y usar variables de broadcast? `broadcast()`\n",
    "\n",
    "\n",
    "`SparkContext.broadcast(value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = sc.broadcast({'83': 'Mezes Park', '84':'Dolores Park'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7efc1b1e74e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aunque podemos instanciar variables de broadcast a partir de archivos; su posible uso se traslapa con la función básica del RDD** 👮\n",
    "\n",
    "\n",
    "\n",
    "Una vez que tenemos objetos de tipo broadcast, los podemos usar con la siguiente sintaxis:\n",
    "\n",
    "`Clase: value()`\n",
    "\n",
    "`Broadcast.value()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dolores Park'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.value['84']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "`unpersist()`\n",
    "\n",
    "Igual que la función anterior de persistencia para RDDs.\n",
    "\n",
    "`Broadcast.unpersist(blocking=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuáles son las ventajas de las variables de Broadcast ?\n",
    "\n",
    "\n",
    "👀\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "# Accumulators\n",
    "\n",
    "Otro tipo de variables compartidas en Spark, son las variables de tipo acumulador. A diferencia del tipo Broadcast, estas pueden actualizar su valor. Especificamente son valores numéricos (enteros o decimales) que se pueden incrementar.\n",
    "\n",
    "Se escriben inicialmente por nuestro Spark Driver y su actualización corresponde a las tareas en los nodos. El valor final del acumulador puede ser obtenido de regreso, generalmente al acabar la app de Spark.\n",
    "\n",
    "Se actualizan solamente una vez por tarea exitosa. Los nodos envian las actualizaciones hacia el Spark Driver, que es el único proceso que puede leer su valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plus_one(x):\n",
    "    global acc\n",
    "    acc += 1\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myrdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.map(lambda x: plus_one(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros procesados: 5\n"
     ]
    }
   ],
   "source": [
    "print('Registros procesados: ' + str(acc.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.map(lambda x: plus_one(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros procesados: 10\n"
     ]
    }
   ],
   "source": [
    "print('Registros procesados: ' + str(acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accumulators `Obj: AccumulatorParam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        dict1 = {}\n",
    "        for i in range(0, len(value)):\n",
    "            dict1[i] = 0\n",
    "        return dict1\n",
    "    # next funct\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in val1.keys():\n",
    "            val1[i] += val2[i]\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([{ 0: 0.3, 1: 0.8, 2: 0.4}, {0: 0.2, 1: 0.4, 2: 0.2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_acc = sc.accumulator({0: 0, 1: 0, 2: 0}, VectorAccumulatorParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapping_fn(x):\n",
    "    global vector_acc\n",
    "    vector_acc += x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1.foreach(mapping_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5, 1: 1.2000000000000002, 2: 0.6000000000000001}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usos para variables tipo Accumulator\n",
    "\n",
    "Generalmente los usaremos para propósitos operacionales, como:\n",
    "\n",
    "- conteo de registros procesados\n",
    "- seguimiento de registros malformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
