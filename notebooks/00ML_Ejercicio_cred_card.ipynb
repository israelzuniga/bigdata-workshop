{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "## Anonymized credit card transactions labeled as fraudulent or genuine\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "\n",
    "Objetivo: Entrenar un clasificador para detectar operaciones fraudulentas.\n",
    "\n",
    "Se estima que al año se realizan 100 **mil millones** de transacciones con CC.\n",
    "\n",
    "\n",
    "Obtenemos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/israelzuniga/dlatam-bigdata-workshop/raw/master/notebooks/data/creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos constantes para re-usarse en la app de Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_PATH = \"./creditcard.csv\"\n",
    "APP_NAME = \"Random Forest Example\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 13579\n",
    "TRAINING_DATA_RATIO = 0.7\n",
    "RF_NUM_TREES = 3\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32\n",
    "RF_MAX_BINS = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .master(SPARK_URL) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferschema = \"true\") \\\n",
    "    .csv(CSV_PATH)\n",
    "\n",
    "print(\"Rows: %d\" % df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que cargamos el CSV, necesitamos transformarlo a un vector y separar datos para entrenamiento y pruebas:\n",
    "\n",
    "https://spark.apache.org/docs/1.1.0/mllib-data-types.html\n",
    "\n",
    "https://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#module-pyspark.mllib.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "transformed_df = df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "print(\"Training set rows: %d\" % training_data.count())\n",
    "print(\"Test set rows: %d\" % test_data.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un clasificador RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from time import *\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "model = RandomForest.trainClassifier(training_data, numClasses=2, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos predicciones y evaluamos el desempeño del clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\n",
    "acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labels_and_predictions)\n",
    "print(\"Area under Precision/Recall (PR) curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "print(\"Area under Receiver Operating Characteristic (ROC) curve: %.3f\" % (metrics.areaUnderROC * 100))\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
